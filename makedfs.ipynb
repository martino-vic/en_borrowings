{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78ef4d1-7abf-45cc-a079-a3243fb3ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e65aab0-0a3d-49f0-b959-0821e3cb0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 0.1.\n",
    "# disambiguate homographs of differnet lgs,\n",
    "# homonyms within the same lg\n",
    "# overall more precise scraping\n",
    "# remove col \"links\" - takes up too much space on disk. better to merge on word+disamb_nr \n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import re\n",
    "import numpy as np\n",
    "import shutil\n",
    "import logging\n",
    "logging.basicConfig(filename='parser.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "class Scrape():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.maxthreads = 100\n",
    "        self.gloss = []\n",
    "        self.ipa = []\n",
    "        self.etym = []\n",
    "        self.l1 = \"\"\n",
    "\n",
    "    def download_url(self, url):\n",
    "        html = get(url)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')        \n",
    "        word = url.rsplit('/', 1)[1]\n",
    "        gloss = []\n",
    "        ipa = []\n",
    "        etym= []\n",
    "        addcut = 0\n",
    "        ety = \"\"\n",
    "        theipa = \"\"\n",
    "        \n",
    "        for h2 in soup.find_all(\"h2\"): #Language headers like \"English\", \"Hawaiian\", \"Maori\"\n",
    "            h2L1 = h2.find(\"span\", {\"id\": self.l1}) #find the heading of our target lg\n",
    "            if h2L1:\n",
    "                for sib in h2L1.parent.next_siblings: #loop through all siblings (\"next sibling doesnt work\")\n",
    "                        \n",
    "                    if sib.name == 'h2': #dont look for siblings beyond the next header (= the next language)\n",
    "                        break\n",
    "\n",
    "                    if sib.name == \"h3\":\n",
    "                     \n",
    "                        if \"Pronunciation\" in sib.text:\n",
    "                            #pro = sib.find_next_sibling(\"ul\")\n",
    "                            for pro in sib.next_siblings:\n",
    "                                if len(ipa)==1:\n",
    "                                    break\n",
    "                                if pro.name == \"ul\":\n",
    "                                    for li in pro.find_all(\"li\"):\n",
    "                                        theipa = li.find(\"span\", {\"class\": \"IPA\"})\n",
    "                                        if theipa:\n",
    "                                            ipa.append(theipa.text)\n",
    "                                            break\n",
    "\n",
    "                                elif pro.name == \"h2\":\n",
    "                                    ipa.append(\"\")\n",
    "                                    break\n",
    "\n",
    "                        if \"Etymology\" in sib.text:\n",
    "                            #glo = sib.find_next_sibling(\"ol\")\n",
    "                            #if glo:\n",
    "                            for glo in sib.next_siblings:\n",
    "                                \n",
    "                                if glo.name == \"p\":                   \n",
    "                                    if \"English\" in glo.text: #some etymons occure twice, make sure to grab the one that is an LW from English\n",
    "                                        try:\n",
    "                                            ety = glo.i.text\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                            \n",
    "                                elif glo.name == \"ol\":\n",
    "                                    try:\n",
    "                                        #gloss.append([i.text for i in glo.find_all(\"li\")])\n",
    "                                        glo = re.sub(r\"\\n\", \", \", glo.text)\n",
    "                                        addcut = glo[50:].find(\" \")\n",
    "                                        if addcut == -1:\n",
    "                                            gloss.append(glo)\n",
    "                                            etym.append(ety)\n",
    "                                        else:\n",
    "                                            gloss.append(glo[:50+addcut])\n",
    "                                            etym.append(ety)\n",
    "                                    except AttributeError:\n",
    "                                        gloss.append(\"\")\n",
    "                                        etym.append(ety)\n",
    "            \n",
    "                                elif glo.name == \"h2\":\n",
    "                                    break\n",
    "                        \n",
    "\n",
    "\n",
    "                if ipa == []:\n",
    "                    ipa = [\"\"]\n",
    "\n",
    "                if gloss == []:\n",
    "                    #if word == \"hana\":\n",
    "                     #   print(\"list empty\")\n",
    "                      #  if etym == []:\n",
    "                       #     print(\"etym also empty\")\n",
    "                        #else:\n",
    "                         #   print(\"Here's the mistake: etym is not empty\")\n",
    "                    for sib in h2L1.parent.next_siblings: #loop through all siblings (\"next sibling doesnt work\")\n",
    "                        if sib.name == \"h2\":\n",
    "                            break\n",
    "                        if sib.name == \"ol\":\n",
    "                            #if word == \"hana\":\n",
    "                             #   print(gloss, etym)\n",
    "                            gloss.append(sib.li.text)\n",
    "                            etym.append(ety)\n",
    "                            #if word == \"hana\":\n",
    "                             #   print(gloss, etym)\n",
    "\n",
    "                            \n",
    "                if gloss == []:\n",
    "                    #if word == \"hana\":\n",
    "                     #   print(\"yup, entered this weird spot here\")\n",
    "                    gloss = [\"\"]\n",
    "                    etym = [\"\"]\n",
    "                \n",
    "                ipa = ipa*len(gloss)\n",
    "                self.gloss.append((gloss, word))\n",
    "                self.ipa.append((ipa, word))\n",
    "                self.etym.append((etym, word))\n",
    "                    \n",
    "    def download_info(self, lg, url_list):\n",
    "        threads = min(self.maxthreads, len(url_list))\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "            executor.map(self.download_url, url_list)\n",
    "\n",
    "    def main(self, lg, url_list):\n",
    "        self.download_info(lg, url_list)\n",
    "\n",
    "def main(lglist):\n",
    "    scr = Scrape()\n",
    "    lglist = [i for i in lglist if i !=\"English\"]\n",
    "    for lg in lglist:\n",
    "        scr.ipa = []\n",
    "        scr.gloss = []\n",
    "        scr.etym = []\n",
    "        scr.l1=re.sub(\" \", \"_\", lg)\n",
    "        lg = scr.l1.lower()\n",
    "        print(lg)\n",
    "        file = f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\{lg}.txt\"\n",
    "        \n",
    "        if \"proto\" in lg or \"ancient\" in lg:\n",
    "            shutil.move(file, f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\proto_lgs\\\\{lg}.txt\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            dflg = pd.read_csv(file, header=None, sep=\"\\n\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            shutil.move(file, f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\empty_lists\\\\{lg}.txt\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "            \n",
    "        dflg.columns = [\"L2_orth\"]\n",
    "        dflg[\"L2_orth\"] = [str(i) for i in dflg[\"L2_orth\"]]\n",
    "        dflg = dflg[~dflg[\"L2_orth\"].str.contains(lg)].reset_index(drop=True)\n",
    "        dflg = dflg.replace(r'^\\s*$', np.nan, regex=True).dropna()     \n",
    "        url_list = [re.sub(\" \", \"_\", f\"https://en.wiktionary.org/wiki/{i}\") for i in dflg[\"L2_orth\"]]\n",
    "        scr.main(lg, url_list)\n",
    "        \n",
    "        dfipa = pd.DataFrame(scr.ipa, columns = ['L2_ipa', 'L2_orth'])\n",
    "        dfgloss = pd.DataFrame(scr.gloss, columns = ['L2_gloss', 'L2_orth'])\n",
    "        dfetym = pd.DataFrame(scr.etym, columns = ['L2_etym', 'L2_orth'])\n",
    "        \n",
    "        dflg = dflg.merge(dfipa, left_on='L2_orth', right_on='L2_orth')\n",
    "        dflg = dflg.merge(dfgloss, left_on='L2_orth', right_on='L2_orth')\n",
    "        dflg = dflg.merge(dfetym, left_on='L2_orth', right_on='L2_orth')\n",
    "            \n",
    "        try: #usually this works, but sometimes there are 5-20 wrong rows\n",
    "            dflg = dflg.explode([\"L2_ipa\", \"L2_gloss\", \"L2_etym\"])\n",
    "        except ValueError: # pad wrong lengths\n",
    "            ip, gl, et = [], [], []\n",
    "            for nr, (i,g,e) in enumerate(zip(dflg[\"L2_ipa\"], dflg[\"L2_gloss\"], dflg[\"L2_etym\"])):\n",
    "                if len(i) == len(g) and len(g) == len(e):\n",
    "                    ip.append(i)\n",
    "                    gl.append(g)\n",
    "                    et.append(e)\n",
    "                else:\n",
    "                    padi = [i[0]]+[\"\"]*(len(g)-1)\n",
    "                    pade = [e[0]]+[\"\"]*(len(g)-1)\n",
    "                    ip.append(padi)\n",
    "                    gl.append(g)\n",
    "                    et.append(pade)\n",
    "                    logging.debug(f'different len in {lg}.txt for gloss {g} in row {nr}. Turned {i} to {padi} and {e} to {pade}')\n",
    "\n",
    "            dflg[\"L2_ipa\"], dflg[\"L2_gloss\"], dflg[\"L2_etym\"] = ip, gl, et\n",
    "            dflg = dflg.explode([\"L2_ipa\", \"L2_gloss\", \"L2_etym\"])\n",
    "\n",
    "        dflg.to_csv(f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\dfs\\\\{lg}.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22b5108d-92f9-4fb8-a6ff-9652b6657357",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "logging.basicConfig(filename='parser.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "def main(dflg):\n",
    "    lg = \"Dutch\"\n",
    "    try:\n",
    "        dflg = dflg.explode([\"L2_ipa\", \"L2_gloss\", \"L2_etym\"])\n",
    "    except ValueError: # pad wrong lengths\n",
    "        ip, gl, et = [], [], []\n",
    "        for nr, (i,g,e) in enumerate(zip(dflg[\"L2_ipa\"], dflg[\"L2_gloss\"], dflg[\"L2_etym\"])):\n",
    "            if len(i) == len(g) and len(g) == len(e):\n",
    "                ip.append(i)\n",
    "                gl.append(g)\n",
    "                et.append(e)\n",
    "            else:\n",
    "                padi = [i[0]]+[\"\"]*(len(g)-1)\n",
    "                pade = [e[0]]+[\"\"]*(len(g)-1)\n",
    "                ip.append(padi)\n",
    "                gl.append(g)\n",
    "                et.append(pade)\n",
    "                logging.debug(f'different len in {lg}.txt for gloss {g} in row {nr}. Turned {i} to {padi} and {e} to {pade}')\n",
    "                \n",
    "        dflg[\"L2_ipa\"], dflg[\"L2_gloss\"], dflg[\"L2_etym\"] = ip, gl, et\n",
    "        dflg = dflg.explode([\"L2_ipa\", \"L2_gloss\", \"L2_etym\"])\n",
    "    return dflg\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dflg = pd.DataFrame({\"L2_ipa\": [[\"a\",\"b\"],[\"d\",\"j\",\"j\",\"j\"],[\"e\",\"f\"]], \"L2_gloss\": [[\"g\",\"h\"],[\"\"],[\"k\",\"l\"]], \"L2_etym\": [[\"m\",\"n\"],[\"o\",\"p\"],[\"q\"]]})\n",
    "    print(main(dflg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3238de42-1922-466c-ace2-1204338c16d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abenlen_ayta\n",
      "abu'_arapesh\n",
      "acatepec_me'phaa\n",
      "aghu_tharrnggala\n",
      "agusan_manobo\n",
      "aka_(central_africa)\n",
      "akkala_sami\n",
      "alcozauca_mixtec\n",
      "alemannic_german\n",
      "algerian_arabic\n",
      "aloápam_zapotec\n",
      "amatlán_zapotec\n",
      "ambala_ayta\n",
      "ambonese_malay\n",
      "american_sign_language\n",
      "amganad_ifugao\n",
      "ampari_dogon\n",
      "ana_tinga_dogon\n",
      "ancient_greek\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\ancient_greek.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\viktor\\appdata\\local\\programs\\python\\python39\\lib\\shutil.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    805\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\ancient_greek.txt' -> 'C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\proto_lgs\\\\ancient_greek.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-46caf1979021>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlglist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlglist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlglist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Norwegian Bokmål\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlglist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-49aae8610597>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(lglist)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"proto\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlg\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"ancient\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\proto_lgs\\\\{lg}.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viktor\\appdata\\local\\programs\\python\\python39\\lib\\shutil.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m             \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viktor\\appdata\\local\\programs\\python\\python39\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viktor\\appdata\\local\\programs\\python\\python39\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;31m# macOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\loanpy11\\\\tests_phd\\\\test_loanfinder\\\\false_positives\\\\lgs\\\\ancient_greek.txt'"
     ]
    }
   ],
   "source": [
    "#run above code\n",
    "import ast\n",
    "\n",
    "with open('lglist_full.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    lglist = ast.literal_eval(f.read())\n",
    "\n",
    "lglist = [i for i in lglist[:lglist.index(\"Norwegian Bokmål\")] if \" \" in i]\n",
    "main(lglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863181f-fc62-44f8-a0db-66ca0d7c861b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html = get(\"https://en.wiktionary.org/wiki/амҵ\")\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "ipa=[]\n",
    "\n",
    "for h2 in soup.find_all(\"h2\"): #Language headers like \"English\", \"Hawaiian\", \"Maori\"\n",
    "    h2L1 = h2.find(\"span\", {\"id\": \"Abkhaz\"}) #find the heading of our target lg\n",
    "    if h2L1:\n",
    "        for sib in h2L1.parent.next_siblings: #loop through all siblings (\"next sibling doesnt work\")\n",
    "\n",
    "            if sib.name == 'h2': #dont look for siblings beyond the next header (= the next language)\n",
    "                break\n",
    "\n",
    "            if sib.name == \"h3\":\n",
    "\n",
    "                if \"Pronunciation\" in sib.text:\n",
    "                    #pro = sib.find_next_sibling(\"ul\")\n",
    "                    for pro in sib.next_siblings:\n",
    "                        if pro.name == \"ul\":\n",
    "                            theipa = pro.li.find(\"span\", {\"class\": \"IPA\"})\n",
    "                            if theipa:\n",
    "                                print(theipa.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbd64f-e8b9-44df-b298-fec4ac975a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr = Scrape()\n",
    "scr.l1 = \"A-Pucikwar\" \n",
    "scr.download_url(\"https://en.wiktionary.org/wiki/burin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d28661a1-8b08-491e-a406-bcb739f860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr = Scrape()\n",
    "scr.l1 = \"Northern_Yukaghir\" \n",
    "scr.download_url(\"https://en.wiktionary.org/wiki/адуо\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cba0b555-4f62-4166-92de-638b2703dba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scr.gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e27cd382-b3ae-48f5-b782-0df80258ae5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [1,2,3,\"4\",5]\n",
    "lst[:lst.index(\"4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53573f0b-dc09-455e-a1c5-7e7c52279632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#development of version 0.1\n",
    "#find out how to disambiguate without preprocessing\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def main(url):\n",
    "    html = get(url)\n",
    "    word = url.rsplit('/', 1)[1]\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    gloss = []\n",
    "    etym= []\n",
    "    ipa = []\n",
    "    addcut = 0\n",
    "    \n",
    "    l1= \"'Are'are\"\n",
    "    for h2 in soup.find_all(\"h2\"): #Language headers like \"English\", \"Hawaiian\", \"Maori\"\n",
    "        h2L1 = h2.find(\"span\", {\"id\": f\"{l1}\"}) #find the heading of our target lg\n",
    "        if h2L1:\n",
    "            for sib in h2L1.parent.next_siblings: #loop through all siblings (\"next sibling doesnt work\")\n",
    "                if sib.name == \"h3\":\n",
    "                    \n",
    "                    if \"Pronunciation\" in sib.text:\n",
    "                        pro = sib.find_next_sibling(\"ul\")\n",
    "                        if pro:\n",
    "                            ipa.append(pro.li.span.text)\n",
    "                        else:\n",
    "                            ipa.append(\"\")\n",
    "                            \n",
    "                    if \"Etymology\" in sib.text:\n",
    "                \n",
    "                        glo = sib.find_next_sibling(\"ol\")\n",
    "                        if glo:\n",
    "                            try:\n",
    "                                #gloss.append([i.text for i in glo.find_all(\"li\")])\n",
    "                                glo = re.sub(r\"\\n\", \", \", glo.text)\n",
    "                                addcut = glo[50:].find(\" \")\n",
    "                                if addcut == -1:\n",
    "                                    gloss.append(glo)\n",
    "                                else:\n",
    "                                    gloss.append(glo[:50+addcut])\n",
    "                            except AttributeError:\n",
    "                                gloss.append(\"\")\n",
    "                        else:\n",
    "                            gloss.append(\"\")\n",
    "                           \n",
    "                        ety = sib.find_next_sibling(\"p\")\n",
    "                        if ety:\n",
    "                            if \"English\" in ety.text: #some etymons occure twice, make sure to grab the one that is an LW from English\n",
    "                                try:\n",
    "                                    etym.append(ety.i.text) #this is the final info we want\n",
    "                                except:\n",
    "                                    etym.append(\"\")\n",
    "                            else:\n",
    "                                etym.append(\"\")\n",
    "                        else:\n",
    "                            etym.append(\"\")\n",
    "                            \n",
    "                elif sib.name == 'h2': #dont look for siblings beyond the next header (= the next language)\n",
    "                    break\n",
    "                    \n",
    "            if ipa == []:\n",
    "                ipa = [\"\"]\n",
    "\n",
    "            if gloss == []:\n",
    "                for sib in h2L1.parent.next_siblings: #loop through all siblings (\"next sibling doesnt work\")\n",
    "                    if sib.name == \"ol\":                \n",
    "                        gloss.append(sib.li.text) \n",
    "                        \n",
    "    ipa = ipa*len(gloss)\n",
    "    gloss, ipa, etym = (gloss, word), (ipa, word), (etym, word)\n",
    "        \n",
    "    return gloss, ipa, etym\n",
    "main(\"https://en.wiktionary.org/wiki/teteku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a4fbb-7a2b-4606-ba5b-0d1efc83bcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35870700-8d9e-4cba-bc34-eec3a124227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 0.0 - works.\n",
    "#a bit buggy: main problem = homonyms\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import re\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "class Scrape():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.maxthreads = 100\n",
    "        self.gloss = []\n",
    "        self.ipa = []\n",
    "\n",
    "    def download_url(self, url):\n",
    "        html = get(url)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        try:\n",
    "            self.gloss.append((soup.ol.li.text, url))\n",
    "        except: \n",
    "            self.gloss.append((\"\", url))\n",
    "        try:\n",
    "            self.ipa.append((soup.find(\"span\", class_=\"IPA\").text, url))\n",
    "        except:\n",
    "            self.ipa.append((\"-\", url))\n",
    "    \n",
    "    def download_info(self, lg_list, url_list):\n",
    "        for lg in lg_list: #delete this line and input lg instead of lg_list. bc I'm looping alredy in main()\n",
    "            threads = min(self.maxthreads, len(url_list))\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "                executor.map(self.download_url, url_list)\n",
    "\n",
    "    def main(self, lg_list, url_list):\n",
    "        self.download_info(lg_list, url_list)\n",
    "\n",
    "def main(lglist):\n",
    "    scr = Scrape()\n",
    "    for lg in lglist:\n",
    "        scr.ipa = []\n",
    "        scr.gloass = []\n",
    "        lg = re.sub(\" \", \"_\", lg).lower()\n",
    "        print(lg)\n",
    "        file = f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\baseline tests\\\\other_lgs\\\\lgs\\\\{lg}.txt\"\n",
    "        \n",
    "        if \"proto\" in lg:\n",
    "            shutil.move(file, f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\baseline tests\\\\other_lgs\\\\lgs\\\\proto_lgs\\\\{lg}.txt\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            dflg = pd.read_csv(file, header=None, sep=\"\\n\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            shutil.move(file, f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\baseline tests\\\\other_lgs\\\\lgs\\\\empty_lists\\\\{lg}.txt\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "            \n",
    "        dflg.columns = [\"L2_orth\"]\n",
    "        dflg[\"L2_orth\"] = [str(i) for i in dflg[\"L2_orth\"]]\n",
    "        dflg[\"L2_orth\"] = [re.sub(f\"Reconstruction:{lg}\\/\", \"\", i) for i in dflg[\"L2_orth\"]]\n",
    "        dflg = dflg[~dflg[\"L2_orth\"].str.contains(lg)].reset_index(drop=True)\n",
    "        dflg[\"L2_link\"] = [re.sub(\" \", \"_\", f\"https://en.wiktionary.org/wiki/{i}#{lg}\") for i in dflg[\"L2_orth\"]]\n",
    "        dflg = dflg.replace(r'^\\s*$', np.nan, regex=True).dropna()\n",
    "        scr.main([lg], dflg[\"L2_link\"])\n",
    "        dfipa = pd.DataFrame(scr.ipa, columns = ['L2_ipa', 'L2_link'])\n",
    "        dfgloss = pd.DataFrame(scr.gloss, columns = ['L2_gloss', 'L2_link'])\n",
    "        dflg = dflg.merge(dfipa, left_on='L2_link', right_on='L2_link')\n",
    "        dflg = dflg.merge(dfgloss, left_on='L2_link', right_on='L2_link')\n",
    "        dflg.to_csv(f\"C:\\\\Users\\\\Viktor\\\\OneDrive\\\\PhD cloud\\\\Vorgehensweisen\\\\baseline tests\\\\other_lgs\\\\dfs\\\\{lg}.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bff78-411e-43f1-a493-58b425e77c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run above code\n",
    "import ast\n",
    "\n",
    "with open('lglist_full.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    lglist = ast.literal_eval(f.read())\n",
    "\n",
    "main(lglist[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
